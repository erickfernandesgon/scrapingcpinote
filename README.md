
# Web Scraping e ETL para Tabelas do BLS

Este projeto realiza **web scraping** de tabelas do site do Bureau of Labor Statistics (BLS), transforma os dados capturados (ETL) e salva os resultados processados como arquivos Excel. O processo √© modularizado em tr√™s partes: scraping, ETL e orquestra√ß√£o.

## Estrutura do Projeto

```plaintext
.
‚îú‚îÄ‚îÄ scrapper.py         # Captura os dados da web e salva como CSV
‚îú‚îÄ‚îÄ etl.py              # Processa o CSV e converte para Excel
‚îú‚îÄ‚îÄ orquestrador.py     # Orquestra o fluxo de scraping e ETL
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/            # Armazena os arquivos CSV brutos
‚îÇ   ‚îú‚îÄ‚îÄ processed/      # Armazena os arquivos Excel processados
‚îî‚îÄ‚îÄ README.md           # Documenta√ß√£o do projeto
```

---

## Fluxo do Projeto

1. **Scraper (`scrapper.py`)**:
   - Captura uma tabela espec√≠fica do site do BLS.
   - Converte a tabela HTML capturada em um arquivo CSV.
   - Salva o arquivo na pasta `data/raw/`.

2. **ETL (`etl.py`)**:
   - L√™ o CSV gerado pelo scraper.
   - Realiza transforma√ß√µes b√°sicas nos dados:
     - Remove linhas vazias.
     - Substitui valores ausentes por zero.
   - Salva os dados processados como Excel na pasta `data/processed/`.

3. **Orquestrador (`orquestrador.py`)**:
   - Coordena as etapas de scraping e ETL.
   - Garante que o fluxo seja executado na ordem correta.

---

## Como Usar

### Pr√©-requisitos

1. **Instale as depend√™ncias**:
   Certifique-se de que voc√™ possui o `Python` instalado. Use o seguinte comando para instalar as depend√™ncias:

   ```bash
   pip install selenium pandas
   ```

2. **Estrutura de pastas**:
   - Certifique-se de que as pastas `data/raw/` e `data/processed/` existem. Se n√£o, crie-as manualmente ou o script far√° isso automaticamente.

### Executando o Script

1. **Rodar o Orquestrador**:
   O `orquestrador.py` √© o ponto de entrada do projeto. Ele chamar√° as fun√ß√µes necess√°rias para realizar o scraping e o ETL.

   ```bash
   python orquestrador.py
   ```

2. **Arquivos Gerados**:
   - O **CSV bruto** ser√° salvo em: `data/raw/tabela_2.csv`.
   - O **Excel processado** ser√° salvo em: `data/processed/tabela_2_processada.xlsx`.

---

## Explica√ß√£o do C√≥digo

### 1. Scraper (`scrapper.py`)

- **Fun√ß√£o Principal**: `executar_scraping(url, classe_tabela, indice_tabela, caminho_saida)`
  - Faz o scraping da tabela HTML com base na URL e salva como CSV.
  - Par√¢metros:
    - `url`: URL do site a ser acessado.
    - `classe_tabela`: Classe CSS usada para localizar as tabelas.
    - `indice_tabela`: √çndice da tabela desejada.
    - `caminho_saida`: Caminho para salvar o CSV.

### 2. ETL (`etl.py`)

- **Fun√ß√£o Principal**: `executar_etl(caminho_csv, caminho_excel)`
  - Processa os dados do CSV e salva como Excel.
  - Par√¢metros:
    - `caminho_csv`: Caminho do CSV bruto.
    - `caminho_excel`: Caminho para salvar o Excel processado.

### 3. Orquestrador (`orquestrador.py`)

- **Fun√ß√£o Principal**: `executar_processo_completo()`
  - Chama o `executar_scraping` para capturar os dados e o `executar_etl` para process√°-los.
  - Configura√ß√µes:
    - URL do site.
    - Classe CSS da tabela.
    - √çndices e caminhos para salvar os arquivos.

---

## Pr√≥ximos Passos

- Adicionar suporte a m√∫ltiplas tabelas.
- Implementar logs detalhados para cada etapa.
- Criar testes automatizados para garantir a confiabilidade do c√≥digo.

---

Qualquer d√∫vida ou sugest√£o, √© s√≥ chamar! üöÄ
